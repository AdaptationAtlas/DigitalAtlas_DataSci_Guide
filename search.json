[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for the Digital Atlas",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This handbook is designed to provide an overview of different data types, tools, and practices to be used when working with the Adaptation Atlas ecosystem of tools and data. It can also be a useful guide for other projects as it describes the use of data on AWS S3 buckets, efficient use of cloud-optimized datasets, and notes on some best practices for data management."
  },
  {
    "objectID": "file-conventions.html",
    "href": "file-conventions.html",
    "title": "2  The Agricultural Adaptation Data naming conventions",
    "section": "",
    "text": "Naming formats should be as consistent as possible, especially across similar data sets and directories. For example:\n    {SCENARIO}_{DATE}_{MODEL}_{VARIABLE}_{aggregation_fn}.extension\nThe naming format used should be stored within the metadata for the data.\nIn general:\n\nUnderscores (“_“) should be used to seperate metadata chunks\nHyphens/Dashes (“-”) should be used to separate dates, date ranges and other pieces of information that are in the same group.\nForward slashes (“/”) should be used to separate directories\nPeriods/Full Stops (“.”) should only be used before a file extension\nDates should be in YYYY-MM-DD format or YYYY\n\nThis is a selection of naming conventions used across atlas data:\n\n\n   TOP_DIRECTORY   SCENARIO FUNCTION ADMIN\n1     boundaries historical      sum  adm0\n2        MapSpam       ssp1     mean  adm1\n3        hazards     ssp126       sd  adm2\n4  vulnerability       ssp2      max  adm3\n5     population     ssp245      min      \n6   productivity       ssp3    total      \n7                    ssp370               \n8                      ssp4               \n9                      ssp5               \n10                   ssp585"
  },
  {
    "objectID": "AWS_config.html#method-1",
    "href": "AWS_config.html#method-1",
    "title": "3  Setting up access to the Amazon S3 bucket",
    "section": "3.1 Method 1",
    "text": "3.1 Method 1\nThe first method of setting up only needs to be configured once per device/profile. This is the suggested method if using a personal device that will access the AWS bucket often. Additional help and information can be found https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html. GDAL, Python and R packages, the AWS CLI, etc. will all detect this login info automatically if it is done correctly.\nSteps:\n\nCreate a directory called .aws in the home directory.\nIn ~/.aws/ create an empty file called ‘credentials’.\n\nIMPORTANT NOTE: This is basically just a text file, but there should be there is no file extension. If the file is named credential.txt, it will not work.\n\nOpen this file with a txt editor and create a [default] profile with the access_key_id and secret_access_key.\nCreate other profiles as needed with other access keys and secret keys.\nAlthough not always required (GDAL presets to AWS_REGION = “us-east-1”), if dealing with buckets in multiple regions or getting an error about the region/a key not being found, a config file specifying the region and return type can be made. Keep the output set to json. NOTE: This is the bucket region, not the region of the user. For the digital-atlas bucket, the region is “us-east-1”\n\n\n3.1.1 Example:\nFile locations:\nLinux or macOS:\n~/.aws/credentials AKA /home/USERNAME/.aws/credentials \n\nWindows:\nC:\\Users\\USERNAME\\.aws\\credentials\nCredentials file:\n[default]\naws_access_key_id=AKIAIOSFODNN7EXAMPLE\naws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\n\n[read_only_user]\naws_access_key_id=AKIAI44QH8DHBEXAMPLE\naws_secret_access_key=je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\n\n[zarr_bucket]\naws_access_key_id=AKIAI46QZ8DHBEXAMPLE\naws_secret_access_key=xl7MjGbClwBF/2hp9Htk/h3gCo7nvbEXAMPLEKEY\nConfig file:\n[default]\nregion=us-east-1\noutput=json\n\n[profile read_only_user]\nregion=us-west-2\noutput=json\n\n[profile zarr_bucket]\nregion=us-west-2\noutput=text"
  },
  {
    "objectID": "AWS_config.html#method-2",
    "href": "AWS_config.html#method-2",
    "title": "3  Setting up access to the Amazon S3 bucket",
    "section": "3.2 Method 2",
    "text": "3.2 Method 2\nThis method sets the AWS details as environmental variables and needs to be reconfigured each time a session ends. This can also be used to override the above ~/.aws/credentials or ~/.aws/config variables if needed. It can be done from within R or Python or through the command line.\nR:\n\nSys.setenv(\n    AWS_ACCESS_KEY_ID = 'AKIAIOSFODNN7EXAMPLE',\n    AWS_SECRET_ACCESS_KEY = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    AWS_REGION = \"us-east-1\"\n    )\n\nPython:\n\nimport os\nos.environ['AWS_ACCESS_KEY_ID'] = 'AKIAIOSFODNN7EXAMPLE'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\nos.environ['AWS_REGION'] = \"us-east-1\"\n\nLinux/macOS shell:\n\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-east-1"
  },
  {
    "objectID": "AWS_basic-use.html",
    "href": "AWS_basic-use.html",
    "title": "4  Basic use of AWS S3 in R",
    "section": "",
    "text": "5 Using AWS in R with S3FS\nFor files that are not geospatial, reading/writing large volumes of data to/from an S3 bucket, or other instances where more flexibility is needed, the S3FS R package is useful. This also provides an interface to change the access permissions of an s3 object. Multiple functions, such as upload and download have an async version which can be used with the ‘future’ package to do uploads in parallel. This should be used with larger uploads to save time. See the bottom of this page for an example of setting up a parallel upload.\nTo list buckets, folders, and files in the s3\n#list buckets\ns3_dir_ls()\n\n#list top-level directories in the bucket\ns3_dir_ls('s3://digital-atlas')\n\n#list files in the hazard directory\ns3_file_ls('s3://digital-atlas/hazards')\nTo copy a file or folder to local device from s3 bucket\n#download a folder\ns3_dir_download('s3://digital-atlas/hazards/cmip6', '~/cmip6')\n#download an individual file\ns3_file_download('s3://digital-atlas/boundaries/atlas-region_admin2_harmonized.gpkg',\n    '~/Downloads/my-download_name.gpkg')\n###Alternative option which can also be used to copy within a bucket###\n#copy a folder\ns3_dir_copy('s3://digital-atlas/hazards', 'hazards')\n#copy an individual file\ns3_file_copy('s3://digital-atlas/boundaries/atlas-region_admin2_harmonized.gpkg',\n    'my-download_name.gpkg')\nTo upload a file or folder from a local device to s3 bucket\n#upload a folder\ns3_dir_upload('~/path/to/my/local/directory', 's3://mybucket/path/to/dir')\n#upload an individual file\ns3_file_upload('~/path/to/my/local/file.txt', 's3://mybucket/path/file.txt')\nTo move a file in the s3 bucket\ns3_file_move('s3://mybucket/path/my_file.csv', 's3://mybucket/newpath/my_file.csv')\nTo change file or directory access permissions\ns3_file_chmod('s3://path/to/a/file.txt', mode = 'private') # files are by default private, only authorized users can read or write\ns3_file_chmod('s3://path/to/a/different/file.txt', mode = 'public-read') # anyone with the link can see/download the data\ns3_file_chmod('s3://path/to/a/different/full-public/file.txt', mode = 'public-read-write') # DANGEROUS as anyone could edit/delete the file\nMore complex access permissions (i.e. specific emails, web domains, etc.) can be set in R using the paws.storage package, the AWS CLI, BOTO3/S3FS for Python, etc. This may also need to be done by setting up cross-origin resource sharing for Javascript and web applications. This can also be retrieved or set in R using a paws.storage function."
  },
  {
    "objectID": "AWS_basic-use.html#using-aws-with-gdal-and-gdal-based-r-tools-terra-stars-sf-etc.",
    "href": "AWS_basic-use.html#using-aws-with-gdal-and-gdal-based-r-tools-terra-stars-sf-etc.",
    "title": "4  Basic use of AWS S3 in R",
    "section": "4.1 Using AWS with GDAL and GDAL based R tools (Terra, Stars, sf, etc.)",
    "text": "4.1 Using AWS with GDAL and GDAL based R tools (Terra, Stars, sf, etc.)\nGDAL has special prefixes to access virtual filesystems like S3 and GCS buckets. For S3, this prefix is ‘/vsis3/’ and more information can be found here. This prefix should be appended to any s3 file path, replacing the ‘s3://’.\n\n4.1.1 Other GDAL configurations\nAn error may occur when accessing public buckets and datasets if you have credentials saved on your device or in your environment. This is because your credentials are being passed to a bucket where the credentials do not exist, even though the data is public. To access a public dataset without passing the aws credentials, set or pass the variable “AWS_NO_SIGN_REQUEST=[YES/NO]” to GDAL. Similarly, you may have multiple AWS profiles for different buckets or permissions saved in your .aws/credentials file (Chapter 1). If you want to use a profile other than the default, set or pass the environmental variable “AWS_PROFILE=value” where value is the name of the profile.\nFor the digital-atlas bucket, a GDAL virtual path this would look like:\n/vsis3/digital-atlas/path/to/my/file.tif\nIn R terra:\n\nlibrary(terra)\ncloud_cog &lt;- rast('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_V_TA-vop_tot.tif')\n\nThis will allow reading or writing of any file format compatible with GDAL. However, cloud-optimized formats such as COG will have the best performance."
  },
  {
    "objectID": "cloud_optim_data.html#cloud-optimized-geotiffs-cogs",
    "href": "cloud_optim_data.html#cloud-optimized-geotiffs-cogs",
    "title": "5  Making the most of Cloud Optimized formats",
    "section": "5.1 Cloud Optimized geoTIFFs (COGs)",
    "text": "5.1 Cloud Optimized geoTIFFs (COGs)\nAs the name implies, this is the cloud-optimized version of a geotiff. It uses the same (.tif) file extension, but it contains extra metadata and internal overviews. These act exactly the same as a normal geotiff and are backward compatible with standard geotiffs. To get the most from this format when using gdal-based tools, GDAL Virtual File Systems should be used to read/write data from the cloud. To do so, the following should be appended to the start of the url, depending on where the data is: - ‘/vsicurl/’ for http/https/ftp or public aws/gcs buckets - ‘/vsis3/’ for files in s3 buckets - ‘/vsigs/’ for files in google cloud storage - ‘/vsiaz/’ for files in azure cloud storage These will work for both raster and vector data and can be used in R, QGIS, GDAL CLI, etc.\n\nlibrary(terra)\n# on private S3 bucket\nprivate_cog &lt;- rast('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_V_TA-vop_tot.tif')\n# on public S3 bucket\nSys.setenv(AWS_NO_SIGN_REQUEST = TRUE)\npublic_cog &lt;- rast(paste('\"/vsis3/copernicus-dem-30m/Copernicus_DSM_COG_10_S90_00_W172_00_DEM/Copernicus_DSM_COG_10_S90_00_W172_00_DEM.tif\"', gdalconfig))\n# from an https link. \nhttps_cog &lt;- rast('/vsicurl/https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/map/ESA_WorldCover_10m_2021_v200_S15E027_Map.tif')\n\n# # same process with stars\nlibrary(stars)\nprivate_s3_cog &lt;- read_stars('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_H_TA-ha_tot.tif', proxy = T)"
  },
  {
    "objectID": "cloud_optim_data.html#zarr",
    "href": "cloud_optim_data.html#zarr",
    "title": "5  Making the most of Cloud Optimized formats",
    "section": "5.2 ZARR",
    "text": "5.2 ZARR\nThis format fills the space of netCDF, HDF5 and similar multi-dimensional data cubes. Currently, the ZARR ecosystem is much more developed for Python; however, there is an active push to bring it to other languages. GDAL has some functionality for working with multi-dimensional arrays and ZARR but it is limited. This limits its use in in R spatial world for the time being. In R, Zarr datasets can be best accessed using the stars package, although with limitations and caveats.\n\n#note that currently read_mdim reads the full data set into memory,\n# so anything with less than 32gb ram will likenly not work\nlibrary(stars)\nSys.setenv(AWS_NO_SIGN_REQUEST = TRUE)\nzarr_store = 'ZARR:\"/vsis3/cmip6-pds/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-ESM4/ssp585/r1i1p1f1/day/psl/gr1/v20180701/\"'\nterra::sds(zarr_store)\n\n# info = gdal_utils(\"mdiminfo\", zarr_store, quiet = TRUE)\n# jsonlite::fromJSON(info)$dimensions\n# zarr &lt;-  read_mdim(zarr_store, count = c(NA, NA, 97820))\n\nThe Python ecosystem is much more developed for ZARR at the time being. Xarray is the primary package for working with ZARR and other multidimensional data.\n\nimport s3fs\nimport xarray as xr #pip install xarray[complete] for use with ZARR data\n\n# Loading the data\ns3 = s3fs.S3FileSystem(profile=\"ca_zarr\") #a profile saved in the .aws/credentials file\ndef open_s3_zarr(zarr_s3path: str, s3: s3fs.core.S3FileSystem) -&gt; xr.Dataset:\n  store = s3fs.S3Map(root=zarr_s3path, s3=s3)\n  return xr.open_zarr(store)\n\nhumidityZR = open_s3_zarr(\n    's3://climate-action-datalake/zone=raw/source=agera5/variable=relativehumidity.zarr/',\n    s3)\n\nX = 7.65\nY = -37.33\n\npoint = humidityZR.sel(lat = X, lon = Y, method = 'nearest')\nvals = point.Relative_Humidity_2m_12h.values\npoint.Relative_Humidity_2m_12h.plot()"
  },
  {
    "objectID": "cloud_optim_data.html#geoparquet",
    "href": "cloud_optim_data.html#geoparquet",
    "title": "5  Making the most of Cloud Optimized formats",
    "section": "5.3 geoparquet",
    "text": "5.3 geoparquet\nThis is one of the main cloud optimized formats for geospatial vector datasets. If GDAL 3.8 is installed it will use the newest geoparquet 1.0.0 spec. The same virtual file system prefixes used for rasters can be used to access these. Before GDAL 3.5 there is no geoparquet support. For Python users, geopandas supports the latest 1.0.0 geoparquet spec.\nFor R users:\n\n# This section is expected to rapidly change upon release of the 'geoparquet-r package.\n\n### Using terra (sf will be similar)\nlibrary(terra)\nprint(paste(\"GDAL version:\", gdal()))\nif (any(grepl(\"Parquet\", gdal(drivers = TRUE)))) {\n  print(\"Parquet driver available\")\n}\n\npq_uri &lt;- '/vsis3/digital-atlas/risk_prototype/data/exposure/crop_ha_adm0_sum.parquet'\ns3_parquet &lt;- vect(pq_uri)\n# or to filter it by extent while reading\naoi = ext(c(-4.5, 51.41, -34.83, -1.6))\next_parquet &lt;- vect(pq_uri, extent = aoi) # filter = X can be used for a vector extent\n# or filter and perform operations on columns through SQL query\n# NOTE: these methods can also be used for any vector format (.shp, .gpkg, etc)\nsql_query &lt;- r\"(\nSELECT \"sum.wheat\" as wheat, admin_name\nFROM  crop_ha_adm0_sum\nWHERE admin_name LIKE '%N%' OR \"sum.wheat\" &gt; 5000\n)\"\n\nqueried_parquet &lt;- vect(pq_uri, query = sql_query)\n\n# to write a parquet\nexample &lt;- vect(ext(c(-4.5, 51.41, -34.83, -1.6)), crs = 'EPSG:4326')\n\nx &lt;- writeVector(example, '/vsis3/s3/path/name.parquet', filetype = 'parquet')\n\n\n### Using geoparquet-R\n# Package currently in development\n\n### Using SFarrow\n# Package is depreciated and it is not encouraged unless it is the only options\n# Note an error may occur if a geoparquet was made with the newest 1.0.0 spec \nlibrary(sfarrow)\n# To return the full parquet in memory\nsf_pq &lt;- sfarrow::st_read_parquet(\"s3://digital-atlas/risk_prototype/data/exposure/crop_ha_adm0_sum.parquet\")\n\n# To access and query the parquet from out of memory first \n# See the next section for more on quering with arrow datasets\nlibrary(arrow)\npq_ds &lt;- arrow::open_dataset('s3://digital-atlas/risk_prototype/data/hazard_mean/annual/haz_means_adm2.parquet')\nfiltered_sf &lt;- pq_ds |&gt; \n    dplyr::filter(iso3 == 'UGA') |&gt;\n    sfarrow::read_sf_dataset(find_geom = T)"
  },
  {
    "objectID": "cloud_optim_data.html#parquetarrow",
    "href": "cloud_optim_data.html#parquetarrow",
    "title": "5  Making the most of Cloud Optimized formats",
    "section": "5.4 PARQUET/ARROW",
    "text": "5.4 PARQUET/ARROW\nThis is the cloud optimized answser to tabular datasets such as csv, tsv, and excel. The Arrow package for R provides the main interface to this data format and it is very well documented. If you want to understand and take advantage of everything this format and package can offer, check out the Arrow R package documentation.\n\n### Using the r Arrow package\nlibrary(arrow)\nuri &lt;- 's3://digital-atlas/risk_prototype/data/hazard_risk_vop_ac/annual/haz_risk_vop_ac_reduced.parquet'\n#to read the full parquet in memory\nparquet_tbl &lt;- read_parquet(uri) #returns a normal dataframe of the parquet\n\n# to access and query the parquet from out of memory\nds &lt;- open_dataset(uri, format = \"parquet\")\nds_scan &lt;- ds$NewScan()\nds_scan$Filter(Expression$field_ref(\"admin0_name\") == \"Kenya\")\nfiltered &lt;- dataset &lt;- ds_scan$Finish()$ToTable() #returns an arrow table\nfiltered_df &lt;- as.data.frame(dataset)\n\n# Arrow was also designed to play well with dplyr verbs on out-of-memory tables\nlibrary(dplyr)\nds &lt;- read_parquet(uri, as_data_frame = FALSE) #open_dataset also works here\nfiltered_df &lt;- ds |&gt;\n                filter(admin0_name == \"Lesotho\") |&gt;\n                collect()\n\n#See the next section to learn how to query these using native AWS s3 methods"
  },
  {
    "objectID": "cloud_optim_data.html#running-sql-queries-on-the-cloud-for-csv-json-and-parquet-datasets",
    "href": "cloud_optim_data.html#running-sql-queries-on-the-cloud-for-csv-json-and-parquet-datasets",
    "title": "5  Making the most of Cloud Optimized formats",
    "section": "5.5 Running SQL queries on the cloud for CSV, JSON, and parquet datasets",
    "text": "5.5 Running SQL queries on the cloud for CSV, JSON, and parquet datasets\nAWS S3 allows users to query data from CSV, Parquet, or JSON files using SQL on the cloud. This allows CSV and JSON files, which are not necissarily considered “cloud-optimized”, to be accessed and queried very quickly from the cloud. This can also be used for parquet files, although the returned data will always be in either csv or json format. This website is a useful help guide to the AWS S3 select SQL syntax.\n\nlibrary(paws.storage)\nbucket &lt;- paws.storage::s3()\n\nsql_query &lt;- \"\nSELECT scenario, admin0_name\nFROM S3Object\nWHERE admin0_name= 'Lesotho'\n\"\n#Note that 'S3Object', not the file path/key, is the table name in \"FROM\"\n\naws_result &lt;- bucket$select_object_content(\n    Bucket = 'digital-atlas',\n    Key = 'MapSpam/raw/',\n    Expression = sql_query, \n    ExpressionType = 'SQL', \n    InputSerialization = list(\n    'CSV' = list(FileHeaderInfo = \"USE\")\n    ),\n  OutputSerialization = list(\n    'CSV'= list(\n      QuoteFields = \"ASNEEDED\"\n    )\n))\n\ndata &lt;- read.csv(text = aws_result$Payload$Records$Payload, header = FALSE)\ndata\n\n### Or query a parquet from S3\n\n# A the query may have a mix of quotes in it, the r\"()\" raw fun. can be useful\n# In this case, 'value' is both a reserved s3 select word and a column name. \n# without \"\" surrounding value it will not be interpreted as a column\n# Some complex queries allowing aggregation are also allowed\nsql_query &lt;- r\"(\nSELECT SUM(\"value\") AS total_vop, AVG(\"value\") AS avg_vop\nFROM S3Object\nWHERE exposure = 'vop' AND crop = 'wheat'\n)\"\n\noutput &lt;- bucket$select_object_content(\n    Bucket = 'digital-atlas',\n    Key = 'risk_prototype/data/exposure/exposure_adm_sum.parquet',\n    Expression = sql_query, \n    ExpressionType = 'SQL', \n    InputSerialization = list(\n    'Parquet' = list()\n    ),\n  OutputSerialization = list(\n    'CSV'= list(\n      QuoteFields = \"ASNEEDED\"\n    )\n))\n\ndata &lt;- read.csv(text = output$Payload$Records$Payload, header = FALSE)\ndata\n\nHere are some example queries which could be of use:\n\n-- Selects all columns where admin0 is Tanzania\nSELECT *\nFROM S3Object\nWHERE admin0_name = 'Tanzania'\n\n\n-- Selects first 5 rows (similar to head())\nSELECT *\nFROM S3Object\nLIMIT 5\n\n-- Calculates the average wheat vop in a dataset.\n-- Again, note that value is in \"\" due to it being a reserved word and a column\nSELECT AVG(\"value\") as avg_wheat_vop, SUM(total_pop) as all_population\nFROM S3Object\nWHERE exposure = 'vop' AND crop = 'wheat'\n\n-- Selects the unique crop names in a dataset\nSELECT DISTINCT crop\nFROM S3Object"
  },
  {
    "objectID": "STAC.html#building-stac-metadata-with-pystac",
    "href": "STAC.html#building-stac-metadata-with-pystac",
    "title": "6  Spatio-Temporal Asset Catalogs",
    "section": "6.1 Building STAC metadata with PySTAC",
    "text": "6.1 Building STAC metadata with PySTAC\nLoad the required packages\n\nimport pystac\nimport datetime as dt\nimport shapely\n#Other useful packages that help automate parts of this process:\n# import riostac #this is a useful package \n# import xarray stac\n\n\n6.1.1 STAC catalogs\nCatalogs are the simplest of the possible STAC specifications to make as it does not have to have any specific spatio-temporal extent to describe them. The example below shows how to build a very basic STAC catalog using pystac.\n\nproductivity_catalog = pystac.Catalog( \n  id=\"crop_productivity_data\",\n  description=\"Modelled data of crop productivity\",\n  title=\"Crop Productivity\"\n  )\n\n\n\n6.1.2 STAC collections\nSTAC collections are very similar to STAC catalogs, except they have extra metadata such as a spatio-temporal extent, keywords, custom fields, and STAC extensions which are explained in more detail in the STAC extension heading. They are a bit more difficult to build due to this extra metadata, but the general process is below.\n\n# Build the temporal extent of the collection\n# the strptime function takes a string time and the format it is in such as %Y-%M-%d\n# STAC requires the time to be in UTC, so the replace function forces it into that format\n# NOTE: that this method will default to YEAR-01-01 if the month and day aren't provided/NA\ntime = ['2000', '2030']\nstart = str(dt.datetime.strptime(time[0], '%Y').replace(tzinfo=datetime.timezone.utc))\nend = str(dt.datetime.strptime(time[1], '%Y').replace(tzinfo=datetime.timezone.utc))\n\n# Build the spatial extent of the collection\n# The bounds can be calculated using the riostac package, rasterio, terra, etc. \nbounds = {'xmin': -180, 'ymin': -90, 'xmax': 180, 'ymax': 90}\nbbox = [bounds['xmin'], bounds['ymin'], bounds['xmax'], bounds['ymax']]\n\n# And now put the whole catalog together\nproductivity_paper_collection = pystac.Collection(id = \"paper1_data\",\n    description = \"data from paper by Todd\",\n    keywords = \"productivity\", \"maize\", \"rice\", \"treated\", \"adaptation\"\n    license = \"CC-BY-4.0\",\n    extent = pystac.Extent(\n        spatial = pystac.SpatialExtent(bboxes=[bbox]),\n        temporal = pystac.TemporalExtent(\n            intervals=[\n            start,\n            end\n            ]\n        )\n    )\n)\n\n\n\n6.1.3 STAC items\n\n# In this example the bbox of the item is the same as the collection,\n# but this is not always the case\nbounds = {'xmin': -180, 'ymin': -90, 'xmax': 180, 'ymax': 90}\nbbox = [bounds['xmin'], bounds['ymin'], bounds['xmax'], bounds['ymax']]\nfootprint = shapely.Polygon([\n            [bounds['xmin'], bounds['ymin']],\n            [bounds['xmin'], bounds['ymax']],\n            [bounds['xmax'], bounds['ymax']],\n            [bounds['xmax'], bounds['ymin']]\n        ])\n\n# Now we set the date of the item\n# we could use 'strptime()' agin if date is a charachter, or set it like this:\ndata_date = dt.datetime(2020, 2, 5).replace(tzinfo=dt.timezone.utc)\n\nitem = pystac.Item(id='maize_productivity.tif',\n                      geometry=footprint,\n                      bbox=bbox,\n                      datetime=data_date, #Can be set to `None` if multiple dates\n                      #start_datetime=start_date, #Optional if multiple dates\n                      #end_datetime=end_date, \n                      properties={\n                          'extraMetadataField': 'value',\n                          'extraMetadataField2': 'value2',\n                          \"unit\": \"kg/ha\",\n                          \"ssp\": \"SSP2-4.5\"\n                      })\n\nMost often there will be more than one item in a collection. It is often useful to wrap the above code into a for loop and append each item to a list of items. ### STAC assets This is the final piece of the STAC spec. An asset holds the paths to the STAC Item, other data, derived datasets, and other useful assets. It offers a lot of flexibility, you can have a single asset for each date, a different asset for each model or crop, etc. depending on what fits best with the data.\n\nasset = pystac.Asset(href=\"s3://bucket/maize_productivity.tif\", \n                     media_type=pystac.MediaType.COG)\nitem.add_asset(\"COG Image\", asset)\n\n# or for multiple\ns3_uris = [\n    \"s3://bucket/maize_productivity_2020-1.tif\",\n    \"s3://bucket/maize_productivity_2020-2.tif\",\n    \"s3://bucket/maize_productivity_2020-3.tif\"\n    ]\n\nfor uri in s3_uris:\n    asset = pystac.Asset(href=uri, media_type=pystac.MediaType.COG)\n    asset_time = uri.split(\"_\")[-1] # get the filename \n    asset_name = f'maize productivity cog {asset_time}'\n    item.add_asset(asset_name, asset)\n\n\n\n6.1.4 STAC extensions\nSTAC extensions can be added to all of the STAC specifications - catalogs, collections, items, and assets. These extensions provide a more formal metadata framework for certain types/aspects of metadata (i.e. raster, projection, or data cube specific metadata).\n\nproj = pystac.extensions.ProjectionExtension.ext(item, add_if_missing=True) #add to the item\n\nproj.apply(epsg=4326)\n\n\n\n6.1.5 Combining them all into a final catalog\n\n# add the item to the collection\nproductivity_paper_collection.add_item(item)\n# use .add_items() if you have multiple items in a list \n\n# Add the collection to the catalog\nproductivity_catalog.add_child(productivity_paper_collection)\n\n# Now just save it\nproductivity_catalog.normalize_and_save(\"~/stac/\", catalog_type=pystac.CatalogType.SELF_CONTAINED)"
  },
  {
    "objectID": "observableHQ.html#import-the-required-modules",
    "href": "observableHQ.html#import-the-required-modules",
    "title": "7  Tips for using ObservableHQ with AWS S3 data",
    "section": "7.1 import the required modules",
    "text": "7.1 import the required modules\n\nAWS = await import(\"https://cdn.skypack.dev/@aws-sdk/client-s3@3.212\")"
  },
  {
    "objectID": "observableHQ.html#acessing-a-private-s3-bucket",
    "href": "observableHQ.html#acessing-a-private-s3-bucket",
    "title": "7  Tips for using ObservableHQ with AWS S3 data",
    "section": "7.2 Acessing a private S3 bucket",
    "text": "7.2 Acessing a private S3 bucket\nBucket credentials need to be passed in secret. While they could be hardcoded into the page code, this posses a massive security risk as anyone could access the credentials. This is one of many methods of passing credentials in through an interface:\n\n\nCode\nviewof accessKeyId = Inputs.password({label: \"Access Key ID\"})\n\nviewof secretAccessKey = Inputs.password({label: \"Secret Access Key\"})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet the other AWS parameters\n\n\nCode\nviewof bucket = Inputs.text({label: \"Bucket\", placeholder: \"Enter bucket name\", value: \"digital-atlas\"});\nviewof aws_region = Inputs.text({label: \"Region\", placeholder: \"Enter AWS region\", value: \"us-east-1\"});\nviewof prefix = Inputs.text({label: \"Key Prefix\", placeholder: \"Enter file/folder prefix to search\", value: \"MapSpam\"});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConnect to aws client\n\ns3 = await new AWS.S3Client({\n    region: aws_region,\n    credentials:{\n      accessKeyId: accessKeyId,\n      secretAccessKey: secretAccessKey}\n    })\n\n\n\n\n\n\nView contents of bucket:\n\n\nCode\nls_objresponse = await s3.send(\n  await new AWS.ListObjectsV2Command({\n    Bucket: bucket,\n    Prefix: prefix,\n    MaxKeys: 100\n  })\n);\n\nInputs.table(ls_objresponse.Contents, {\n  columns: [\"Key\", \"LastModified\", \"Size\"]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieve a full object from the bucket:\n\nfullresponse = await s3.send(\n  new AWS.GetObjectCommand({\n    Key: \"risk_prototype/data/hazard_risk_vop/annual/haz_risk_vop_any_adm0_severe.parquet\",\n    Bucket: \"digital-atlas\"\n  })\n);\ntable = fullresponse.Body.transformToByteArray()"
  },
  {
    "objectID": "observableHQ.html#query-a-parquet-and-retrieve-the-result-as-a-json-using-s3-select",
    "href": "observableHQ.html#query-a-parquet-and-retrieve-the-result-as-a-json-using-s3-select",
    "title": "7  Tips for using ObservableHQ with AWS S3 data",
    "section": "7.3 Query a parquet and retrieve the result as a json using S3 Select",
    "text": "7.3 Query a parquet and retrieve the result as a json using S3 Select\n\naws_sqlQuery = `\n  SELECT *\n  FROM S3Object \n  WHERE admin1_name IS NULL \n  AND admin2_name IS NULL \n  and exposure = 'vop'`;\n\nselectresponse = await s3.send(\n  new AWS.SelectObjectContentCommand({\n    ExpressionType: \"SQL\",\n    Expression: aws_sqlQuery,\n    InputSerialization: {\n      Parquet: {}\n    },\n    OutputSerialization: {\n      JSON: {}\n    },\n    Key: \"risk_prototype/data/exposure/exposure_adm_sum.parquet\",\n    Bucket: \"digital-atlas\"\n  })\n); \n\njson_out = {\n  let jsonData = \"\";\n  const events = selectresponse.Payload;\n  for await (const event of events) {\n    if (event.Records) {\n      // event.Records.Payload is a buffer containing\n      // a single record, partial records, or multiple records\n      // it is a utf8 buffer array, so it needs to be decoded and cleaned\n      jsonData += new TextDecoder().decode(event.Records.Payload);\n    }\n  }\n  const dataArray = jsonData.split('\\n').filter(Boolean);\n  const parsedData = dataArray.map(JSON.parse);\n  return parsedData;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView it in a table:\n\n\nCode\nInputs.table(json_out, {\n  columns: [\"admin0_name\", \"crop\", \"value\"]\n});"
  },
  {
    "objectID": "observableHQ.html#this-can-also-be-done-using-duckdb-without-the-aws-sdk-import",
    "href": "observableHQ.html#this-can-also-be-done-using-duckdb-without-the-aws-sdk-import",
    "title": "7  Tips for using ObservableHQ with AWS S3 data",
    "section": "7.4 This can also be done using duckdb without the aws-sdk import",
    "text": "7.4 This can also be done using duckdb without the aws-sdk import\nConnect the DuckDB database to the S3 bucket\n\ns3_duckdb = {\n  const con = await DuckDBClient.of();\n  await con.query(`SET s3_region='${aws_region}'`);\n  await con.query(`SET s3_access_key_id='${accessKeyId}'`);\n  await con.query(`SET s3_secret_access_key='${secretAccessKey}'`);\n  return con;\n}\n\n\n\n\n\n\nQuery the data directly from the S3 bucket and view the result\n\nsqlQuery = `\nSELECT *\nFROM read_parquet('s3://digital-atlas/risk_prototype/data/exposure/exposure_adm_sum.parquet')\nWHERE exposure = 'vop' \nAND admin2_name IS NULL \nAND admin1_name IS NULL\nAND crop = 'wheat'`;\n\ndata = s3_duckdb.query(sqlQuery);\n\nInputs.table(data, {\n  columns: [\"admin0_name\", \"crop\", \"value\", 'exposure']\n});"
  }
]