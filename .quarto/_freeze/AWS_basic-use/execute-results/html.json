{
  "hash": "b4f66ac42009dfb6c048ce998f9dfa8c",
  "result": {
    "markdown": "---\nexecute:\n  eval: false\n---\n\n\n# Basic use of AWS S3 in R\nThis capter introduces the basics of working with the S3 bucket in R.\n\n## Using AWS with GDAL and GDAL based R tools (Terra, Stars, sf, etc.)\n\nGDAL has special prefixes to access virtual filesystems like S3 and GCS buckets. For S3, this prefix is '/vsis3/' and more information can be found [here](https://gdal.org/user/virtual_file_systems.html#vsis3). This prefix should be appended to any s3 file path, replacing the 's3://'.\n\n### Other GDAL configurations\n\nAn error may occur when accessing public buckets and datasets if you have credentials saved on your device or in your environment. This is because your credentials are being passed to a bucket where the credentials do not exist, even though the data is public. To access a public dataset without passing the aws credentials, set or pass the variable \"AWS_NO_SIGN_REQUEST=\\[YES/NO\\]\" to GDAL. Similarly, you may have multiple AWS profiles for different buckets or permissions saved in your .aws/credentials file (Chapter 1). If you want to use a profile other than the default, set or pass the environmental variable \"AWS_PROFILE=value\" where value is the name of the profile.\n\nFor the digital-atlas bucket, a GDAL virtual path this would look like:\n\n```         \n/vsis3/digital-atlas/path/to/my/file.tif\n```\n\nIn R terra:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\ncloud_cog <- rast('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_V_TA-vop_tot.tif')\n```\n:::\n\n\nThis will allow reading or writing of any file format compatible with GDAL. However, cloud-optimized formats such as COG will have the best performance.\n\n# Using AWS in R with S3FS\n\nFor files that are not geospatial, reading/writing large volumes of data to/from an S3 bucket, or other instances where more flexibility is needed, the [S3FS R package](https://dyfanjones.github.io/s3fs/) is useful. This also provides an interface to change the access permissions of an s3 object. Multiple functions, such as upload and download have an async version which can be used with the 'future' package to do uploads in parallel. This should be used with larger uploads to save time. See the bottom of this page for an example of setting up a parallel upload.\n\nTo list buckets, folders, and files in the s3\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#list buckets\ns3_dir_ls()\n\n#list top-level directories in the bucket\ns3_dir_ls('s3://digital-atlas')\n\n#list files in the hazard directory\ns3_file_ls('s3://digital-atlas/hazards')\n```\n:::\n\n\nTo copy a file or folder to local device from s3 bucket\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#download a folder\ns3_dir_download('s3://digital-atlas/hazards/cmip6', '~/cmip6')\n#download an individual file\ns3_file_download('s3://digital-atlas/boundaries/atlas-region_admin2_harmonized.gpkg',\n    '~/Downloads/my-download_name.gpkg')\n###Alternative option which can also be used to copy within a bucket###\n#copy a folder\ns3_dir_copy('s3://digital-atlas/hazards', 'hazards')\n#copy an individual file\ns3_file_copy('s3://digital-atlas/boundaries/atlas-region_admin2_harmonized.gpkg',\n    'my-download_name.gpkg')\n```\n:::\n\n\nTo upload a file or folder from a local device to s3 bucket\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#upload a folder\ns3_dir_upload('~/path/to/my/local/directory', 's3://mybucket/path/to/dir')\n#upload an individual file\ns3_file_upload('~/path/to/my/local/file.txt', 's3://mybucket/path/file.txt')\n```\n:::\n\n\nTo move a file in the s3 bucket\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_file_move('s3://mybucket/path/my_file.csv', 's3://mybucket/newpath/my_file.csv')\n```\n:::\n\n\nTo change file or directory access permissions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns3_file_chmod('s3://path/to/a/file.txt', mode = 'private') # files are by default private, only authorized users can read or write\ns3_file_chmod('s3://path/to/a/different/file.txt', mode = 'public-read') # anyone with the link can see/download the data\ns3_file_chmod('s3://path/to/a/different/full-public/file.txt', mode = 'public-read-write') # DANGEROUS as anyone could edit/delete the file\n```\n:::\n\n\nMore complex access permissions (i.e. specific emails, web domains, etc.) can be set in R using the [paws.storage package](https://www.paws-r-sdk.com/docs/s3_put_bucket_acl/), the AWS CLI, BOTO3/S3FS for Python, etc. This may also need to be done by setting up [cross-origin resource sharing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html) for Javascript and web applications. This can also be retrieved or set in R using a [paws.storage function](https://www.paws-r-sdk.com/docs/s3_get_bucket_cors/).\n\n### Use case 1 - Transfer a tif to s3 and convert it to a Cloud-optimized GeoTIFF\n\nThis is to send a single cog, but it can be easily turned into a for loop or a function for use with lapply and friends\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The easy way is to:\nx <- rast(paste0(\"~/local/path/to/file.tif\"))\nwriteRaster(x, '/vsis3/digital-atlas/path/to/where/i/want/file_cog.tif', filetype = \"COG\", overwrite = T, gdal=c(\"COMPRESS=LZW\",of=\"COG\"))\n# However, sometimes this throws an error. If that happens, this method should work:\nx <- rast(paste0(\"~/local/path/to/file.tif\"))\nwriteRaster(x, paste0(tmp_d, \"/cog_dir/temp_cog.tif\"), filetype = \"COG\", overwrite = T) #convert to cog in temp dir\ns3_file_upload(paste0(tmp_d, \"/cog_dir/temp_cog.tif\"), \"s3://digital-atlas/path/to/where/i/want/file_cog.tif\")\nunlink(paste0(tmp_d, \"/cog_dir/temp_cog.tif\"))\n```\n:::\n\n\n### Use case 2 - Upload a directory to the aws bucket in parallel\n\n::: {.cell}\n\n```{.r .cell-code}\nfuture::availableCores()\nplan('multicore', workers = 5)\n\nfuture <- s3_dir_upload_async(\n  \"~/local/path/to/productivity/data\",\n  's3://digital-atlas/productivity/data/',\n  max_batch = fs_bytes(\"6MB\"),\n  )\n\nupload <- value(future)\n```\n:::\n",
    "supporting": [
      "AWS_basic-use_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}