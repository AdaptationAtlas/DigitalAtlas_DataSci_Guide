{
  "hash": "90a5172471afb9b8f0f9ceb9cce17629",
  "result": {
    "markdown": "---\nexecute:\n  eval: false\n---\n\n\n# Making the most of Cloud Optimized formats\nAn excellent introduction to cloud-native geospatial formats can be found [here](https://guide.cloudnativegeo.org/).\nThese cloud optimized formats provide the benefit of parallel and partial reading from the data source, meaning that an area of interest can be a subset from a raster file or an attribute or specific geometry can be extracted from a vector without the need to download and process the entire dataset. \n\nThis chapter is designed to provide an introduction to using some of these formats in R and Python. Conveniently, many of these formats are processed the same way their non-cloud optimized counterparts would be. It should be noted that these cloud optimized formats are still very young, and many tools and functions are still being developed for use with them. This, alongside a very active community, means that this chapter of the document will be very dynamic as new tools become available and formats become more developed. \n\n## Cloud Optimized geoTIFFs (COGs)\nAs the name implies, this is the cloud-optimized version of a geotiff. It uses the same (.tif) file extension, but it contains extra metadata and internal overviews. These act exactly the same as a normal geotiff and are backward compatible with standard geotiffs. To get the most from this format when using gdal-based tools, [GDAL Virtual File Systems](https://gdal.org/user/virtual_file_systems.html) should be used to read/write data from the cloud. To do so, the following should be appended to the start of the url, depending on where the data is:\n - '/vsicurl/' for http/https/ftp or public aws/gcs buckets\n - '/vsis3/' for files in s3 buckets\n - '/vsigs/' for files in google cloud storage\n - '/vsiaz/' for files in azure cloud storage\nThese will work for both raster and vector data and can be used in R, QGIS, GDAL CLI, etc. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\n# on private S3 bucket\nprivate_cog <- rast('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_V_TA-vop_tot.tif')\n# on public S3 bucket\nSys.setenv(AWS_NO_SIGN_REQUEST = TRUE)\npublic_cog <- rast(paste('\"/vsis3/copernicus-dem-30m/Copernicus_DSM_COG_10_S90_00_W172_00_DEM/Copernicus_DSM_COG_10_S90_00_W172_00_DEM.tif\"', gdalconfig))\n# from an https link. \nhttps_cog <- rast('/vsicurl/https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/map/ESA_WorldCover_10m_2021_v200_S15E027_Map.tif')\n\n# # same process with stars\nlibrary(stars)\nprivate_s3_cog <- read_stars('/vsis3/digital-atlas/MapSpam/intermediate/spam2017V2r3_SSA_H_TA-ha_tot.tif', proxy = T)\n```\n:::\n\n\n## ZARR\nThis format fills the space of netCDF, HDF5 and similar multi-dimensional data cubes. Currently, the ZARR ecosystem is much more developed for Python; however, there is an active push to bring it to other languages. GDAL has some functionality for working with multi-dimensional arrays and ZARR but it is limited. This limits its use in in R spatial world for the time being. In R, Zarr datasets can be best accessed using the stars package, although with limitations and caveats. \n\n::: {.cell}\n\n```{.r .cell-code}\n#note that currently read_mdim reads the full data set into memory,\n# so anything with less than 32gb ram will likenly not work\nlibrary(stars)\nSys.setenv(AWS_NO_SIGN_REQUEST = TRUE)\nzarr_store = 'ZARR:\"/vsis3/cmip6-pds/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-ESM4/ssp585/r1i1p1f1/day/psl/gr1/v20180701/\"'\nterra::sds(zarr_store)\n\n# info = gdal_utils(\"mdiminfo\", zarr_store, quiet = TRUE)\n# jsonlite::fromJSON(info)$dimensions\n# zarr <-  read_mdim(zarr_store, count = c(NA, NA, 97820))\n```\n:::\n\n\nThe Python ecosystem is much more developed for ZARR at the time being. Xarray is the primary package for working with ZARR and other multidimensional data. \n\n::: {.cell python.reticulate='false'}\n\n```{.python .cell-code}\nimport s3fs\nimport xarray as xr #pip install xarray[complete] for use with ZARR data\n\n# Loading the data\ns3 = s3fs.S3FileSystem(profile=\"ca_zarr\") #a profile saved in the .aws/credentials file\ndef open_s3_zarr(zarr_s3path: str, s3: s3fs.core.S3FileSystem) -> xr.Dataset:\n  store = s3fs.S3Map(root=zarr_s3path, s3=s3)\n  return xr.open_zarr(store)\n\nhumidityZR = open_s3_zarr(\n    's3://climate-action-datalake/zone=raw/source=agera5/variable=relativehumidity.zarr/',\n    s3)\n\nX = 7.65\nY = -37.33\n\npoint = humidityZR.sel(lat = X, lon = Y, method = 'nearest')\nvals = point.Relative_Humidity_2m_12h.values\npoint.Relative_Humidity_2m_12h.plot()\n```\n:::\n\n\n## geoparquet\nThis is one of the main cloud optimized formats for geospatial vector datasets. If GDAL 3.8 is installed it will use the newest geoparquet 1.0.0 spec. The same virtual file system prefixes used for rasters can be used to access these. **Before GDAL 3.5 there is no geoparquet support.** For Python users, geopandas supports the latest 1.0.0 geoparquet spec.\n\nFor R users:\n\n::: {.cell}\n\n```{.r .cell-code}\n# This section is expected to rapidly change upon release of the 'geoparquet-r package.\n\n### Using terra (sf will be similar)\nlibrary(terra)\nprint(paste(\"GDAL version:\", gdal()))\nif (any(grepl(\"Parquet\", gdal(drivers = TRUE)))) {\n  print(\"Parquet driver available\")\n}\n\npq_uri <- '/vsis3/digital-atlas/risk_prototype/data/exposure/crop_ha_adm0_sum.parquet'\ns3_parquet <- vect(pq_uri)\n# or to filter it by extent while reading\naoi = ext(c(-4.5, 51.41, -34.83, -1.6))\next_parquet <- vect(pq_uri, extent = aoi) # filter = X can be used for a vector extent\n# or filter and perform operations on columns through SQL query\n# NOTE: these methods can also be used for any vector format (.shp, .gpkg, etc)\nsql_query <- r\"(\nSELECT \"sum.wheat\" as wheat, admin_name\nFROM  crop_ha_adm0_sum\nWHERE admin_name LIKE '%N%' OR \"sum.wheat\" > 5000\n)\"\n\nqueried_parquet <- vect(pq_uri, query = sql_query)\n\n# to write a parquet\nexample <- vect(ext(c(-4.5, 51.41, -34.83, -1.6)), crs = 'EPSG:4326')\n\nx <- writeVector(example, '/vsis3/s3/path/name.parquet', filetype = 'parquet')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Using geoparquet-R\n# Package currently in development\n\n### Using SFarrow\n# Package is depreciated and it is not encouraged unless it is the only options\n# Note an error may occur if a geoparquet was made with the newest 1.0.0 spec \nlibrary(sfarrow)\n# To return the full parquet in memory\nsf_pq <- sfarrow::st_read_parquet(\"s3://digital-atlas/risk_prototype/data/exposure/crop_ha_adm0_sum.parquet\")\n\n# To access and query the parquet from out of memory first \n# See the next section for more on quering with arrow datasets\nlibrary(arrow)\npq_ds <- arrow::open_dataset('s3://digital-atlas/risk_prototype/data/hazard_mean/annual/haz_means_adm2.parquet')\nfiltered_sf <- pq_ds |> \n    dplyr::filter(iso3 == 'UGA') |>\n    sfarrow::read_sf_dataset(find_geom = T)\n```\n:::\n\n\n## PARQUET/ARROW\nThis is the cloud optimized answser to tabular datasets such as csv, tsv, and excel. The Arrow package for R provides the main interface to this data format and it is very well documented. If you want to understand and take advantage of everything this format and package can offer, check out the [Arrow R package documentation](https://arrow.apache.org/docs/r/index.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Using the r Arrow package\nlibrary(arrow)\nuri <- 's3://digital-atlas/risk_prototype/data/hazard_risk_vop_ac/annual/haz_risk_vop_ac_reduced.parquet'\n#to read the full parquet in memory\nparquet_tbl <- read_parquet(uri) #returns a normal dataframe of the parquet\n\n# to access and query the parquet from out of memory\nds <- open_dataset(uri, format = \"parquet\")\nds_scan <- ds$NewScan()\nds_scan$Filter(Expression$field_ref(\"admin0_name\") == \"Kenya\")\nfiltered <- dataset <- ds_scan$Finish()$ToTable() #returns an arrow table\nfiltered_df <- as.data.frame(dataset)\n\n# Arrow was also designed to play well with dplyr verbs on out-of-memory tables\nlibrary(dplyr)\nds <- read_parquet(uri, as_data_frame = FALSE) #open_dataset also works here\nfiltered_df <- ds |>\n                filter(admin0_name == \"Lesotho\") |>\n                collect()\n\n#See the next section to learn how to query these using native AWS s3 methods\n```\n:::\n\n\n## Running SQL queries on the cloud for CSV, JSON, and parquet datasets\nAWS S3 allows users to query data from CSV, Parquet, or JSON files using SQL on the cloud. This allows CSV and JSON files, which are not necissarily considered \"cloud-optimized\", to be accessed and queried very quickly from the cloud. This can also be used for parquet files, although the returned data will always be in either csv or json format. [This website](https://nebius.ai/docs/storage/concepts/s3-select-language) is a useful help guide to the AWS S3 select SQL syntax. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(paws.storage)\nbucket <- paws.storage::s3()\n\nsql_query <- \"\nSELECT scenario, admin0_name\nFROM S3Object\nWHERE admin0_name= 'Lesotho'\n\"\n#Note that 'S3Object', not the file path/key, is the table name in \"FROM\"\n\naws_result <- bucket$select_object_content(\n    Bucket = 'digital-atlas',\n    Key = 'MapSpam/raw/',\n    Expression = sql_query, \n    ExpressionType = 'SQL', \n    InputSerialization = list(\n    'CSV' = list(FileHeaderInfo = \"USE\")\n    ),\n  OutputSerialization = list(\n    'CSV'= list(\n      QuoteFields = \"ASNEEDED\"\n    )\n))\n\ndata <- read.csv(text = aws_result$Payload$Records$Payload, header = FALSE)\ndata\n\n### Or query a parquet from S3\n\n# A the query may have a mix of quotes in it, the r\"()\" raw fun. can be useful\n# In this case, 'value' is both a reserved s3 select word and a column name. \n# without \"\" surrounding value it will not be interpreted as a column\n# Some complex queries allowing aggregation are also allowed\nsql_query <- r\"(\nSELECT SUM(\"value\") AS total_vop, AVG(\"value\") AS avg_vop\nFROM S3Object\nWHERE exposure = 'vop' AND crop = 'wheat'\n)\"\n\noutput <- bucket$select_object_content(\n    Bucket = 'digital-atlas',\n    Key = 'risk_prototype/data/exposure/exposure_adm_sum.parquet',\n    Expression = sql_query, \n    ExpressionType = 'SQL', \n    InputSerialization = list(\n    'Parquet' = list()\n    ),\n  OutputSerialization = list(\n    'CSV'= list(\n      QuoteFields = \"ASNEEDED\"\n    )\n))\n\ndata <- read.csv(text = output$Payload$Records$Payload, header = FALSE)\ndata\n```\n:::\n\n\nHere are some example queries which could be of use:\n\n::: {.cell}\n\n```{.sql .cell-code}\n-- Selects all columns where admin0 is Tanzania\nSELECT *\nFROM S3Object\nWHERE admin0_name = 'Tanzania'\n\n\n-- Selects first 5 rows (similar to head())\nSELECT *\nFROM S3Object\nLIMIT 5\n\n-- Calculates the average wheat vop in a dataset.\n-- Again, note that value is in \"\" due to it being a reserved word and a column\nSELECT AVG(\"value\") as avg_wheat_vop, SUM(total_pop) as all_population\nFROM S3Object\nWHERE exposure = 'vop' AND crop = 'wheat'\n\n-- Selects the unique crop names in a dataset\nSELECT DISTINCT crop\nFROM S3Object\n```\n:::",
    "supporting": [
      "cloud_optim_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}